{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CFST MinerU 批量解析\n",
    "\n",
    "使用 MinerU v2.7+ 批量解析 CFST 论文 PDF (300+ 篇)。\n",
    "\n",
    "**流程**:\n",
    "1. PDF 从 Google Drive 读取\n",
    "2. 解析结果保存到 Colab 本地 `/content/parsed_output/`\n",
    "3. 完成后上传到 Google Drive 或下载 zip\n",
    "\n",
    "**前置条件**:\n",
    "- Runtime → Change runtime type → T4 GPU\n",
    "- Google Drive `/cfst-extractor/pdfs/` 中放入所有 PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: 安装 MinerU + 挂载 Drive\n",
    "!pip install \"mineru[pipeline] @ git+https://github.com/opendatalab/MinerU.git\" -q 2>&1 | tail -5\n",
    "\n",
    "import importlib.metadata\n",
    "print(f'mineru: {importlib.metadata.version(\"mineru\")}')\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os, json, glob, subprocess, time, shutil\n",
    "\n",
    "# 路径配置\n",
    "PDF_DIR = '/content/drive/MyDrive/cfst-extractor/pdfs'\n",
    "OUTPUT_DIR = '/content/parsed_output'  # 本地输出\n",
    "LOG_FILE = '/content/parse_log.jsonl'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "pdf_paths = sorted(glob.glob(f'{PDF_DIR}/*.pdf'))\n",
    "print(f'\\n找到 {len(pdf_paths)} 篇 PDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: 检查 GPU + 配置环境\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f'GPU: {gpu_name} ({gpu_mem:.1f} GB)')\n",
    "else:\n",
    "    raise RuntimeError('No GPU! Runtime -> Change runtime type -> T4 GPU')\n",
    "\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "os.environ['MINERU_DEVICE_MODE'] = 'cuda'\n",
    "print('环境配置完成')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: 批量解析 (支持断点续传)\ndef safe_name(name):\n    \"\"\"清理文件名中的特殊字符\"\"\"\n    return name.replace('\\uf03a', '_').replace(':', '_').replace('/', '_')\n\ndef is_parsed(output_dir):\n    \"\"\"检查是否已解析完成\"\"\"\n    if not os.path.exists(output_dir):\n        return False\n    md_files = glob.glob(f'{output_dir}/**/*.md', recursive=True)\n    json_files = glob.glob(f'{output_dir}/**/*content_list*.json', recursive=True)\n    return bool(md_files and json_files)\n\ntotal = len(pdf_paths)\nparsed, skipped, failed = 0, 0, 0\nfailed_papers = []\n\nprint(f'=== 批量解析 {total} 篇 PDF ===')\nprint(f'输出目录: {OUTPUT_DIR}')\nprint()\n\nfor i, pdf_path in enumerate(pdf_paths):\n    pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]\n    output_dir = f'{OUTPUT_DIR}/{safe_name(pdf_name)}'\n    \n    # 断点续传: 跳过已完成的\n    if is_parsed(output_dir):\n        skipped += 1\n        continue\n    \n    print(f'[{i+1}/{total}] {pdf_name[:60]}...', end=' ', flush=True)\n    start = time.time()\n    \n    try:\n        result = subprocess.run(\n            ['mineru', '-p', pdf_path, '-o', output_dir, '-m', 'auto', '-b', 'pipeline',\n             '--f_draw_layout_bbox', 'false',   # 不生成 *_layout.pdf\n             '--f_draw_span_bbox', 'false',     # 不生成 *_span.pdf\n             '--f_dump_orig_pdf', 'false',      # 不生成 *_origin.pdf\n             '--f_dump_middle_json', 'false',   # 不生成 *_middle.json\n             '--f_dump_model_output', 'false',  # 不生成 *_model.json\n            ],\n            capture_output=True, text=True, timeout=600\n        )\n        elapsed = time.time() - start\n        \n        if result.returncode == 0 and is_parsed(output_dir):\n            parsed += 1\n            print(f'OK ({elapsed:.0f}s)')\n            with open(LOG_FILE, 'a') as f:\n                f.write(json.dumps({'paper': pdf_name, 'status': 'ok', 'time_s': round(elapsed, 1)}) + '\\n')\n        else:\n            failed += 1\n            failed_papers.append(pdf_name)\n            print(f'FAIL')\n            with open(LOG_FILE, 'a') as f:\n                f.write(json.dumps({'paper': pdf_name, 'status': 'fail', 'error': result.stderr[-500:]}) + '\\n')\n    except subprocess.TimeoutExpired:\n        failed += 1\n        failed_papers.append(pdf_name)\n        print('TIMEOUT')\n    except Exception as e:\n        failed += 1\n        failed_papers.append(pdf_name)\n        print(f'ERROR: {e}')\n\nprint(f'\\n=== 完成 ===')\nprint(f'成功: {parsed} | 跳过: {skipped} | 失败: {failed}')\nif failed_papers:\n    print(f'\\n失败列表:')\n    for p in failed_papers[:20]:\n        print(f'  - {p}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: 验证输出统计\n",
    "print('=== 输出验证 ===')\n",
    "ok_count, incomplete_count, missing_count = 0, 0, 0\n",
    "\n",
    "for pdf_path in pdf_paths:\n",
    "    pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "    output_dir = f'{OUTPUT_DIR}/{safe_name(pdf_name)}'\n",
    "    \n",
    "    if is_parsed(output_dir):\n",
    "        ok_count += 1\n",
    "    elif os.path.exists(output_dir):\n",
    "        incomplete_count += 1\n",
    "        print(f'  [INCOMPLETE] {pdf_name[:60]}')\n",
    "    else:\n",
    "        missing_count += 1\n",
    "        print(f'  [MISSING] {pdf_name[:60]}')\n",
    "\n",
    "print(f'\\n统计: OK={ok_count} | INCOMPLETE={incomplete_count} | MISSING={missing_count}')\n",
    "\n",
    "# 输出目录大小\n",
    "total_size = sum(os.path.getsize(os.path.join(dp, f)) \n",
    "                 for dp, dn, fn in os.walk(OUTPUT_DIR) for f in fn)\n",
    "print(f'输出目录大小: {total_size / 1024**3:.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: 上传到 Google Drive\n",
    "DRIVE_OUTPUT = '/content/drive/MyDrive/cfst-extractor/parsed'\n",
    "os.makedirs(DRIVE_OUTPUT, exist_ok=True)\n",
    "\n",
    "print(f'上传到: {DRIVE_OUTPUT}')\n",
    "print('复制中...')\n",
    "\n",
    "uploaded = 0\n",
    "for item in os.listdir(OUTPUT_DIR):\n",
    "    src = os.path.join(OUTPUT_DIR, item)\n",
    "    dst = os.path.join(DRIVE_OUTPUT, item)\n",
    "    if os.path.isdir(src) and not os.path.exists(dst):\n",
    "        shutil.copytree(src, dst)\n",
    "        uploaded += 1\n",
    "\n",
    "print(f'上传完成: {uploaded} 个目录')\n",
    "\n",
    "# 复制日志\n",
    "if os.path.exists(LOG_FILE):\n",
    "    shutil.copy(LOG_FILE, f'{DRIVE_OUTPUT}/../parse_log.jsonl')\n",
    "    print('日志已复制')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: 下载为 zip (可选)\n",
    "ZIP_PATH = '/content/cfst_parsed.zip'\n",
    "\n",
    "print('打包中...')\n",
    "!cd {OUTPUT_DIR} && zip -r {ZIP_PATH} . -q\n",
    "\n",
    "zip_size = os.path.getsize(ZIP_PATH) / 1024**3\n",
    "print(f'ZIP 大小: {zip_size:.2f} GB')\n",
    "\n",
    "from google.colab import files\n",
    "files.download(ZIP_PATH)\n",
    "print('下载已开始')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}